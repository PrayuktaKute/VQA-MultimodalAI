VISUAL QUESTION ANSWERING (VQA) WEB APP

A simple and interactive Visual Question Answering (VQA) web application built using Gradio and Hugging Face Transformers.
The app allows users to upload an image, ask a natural language question about it, and receive an answer generated by a pre-trained multimodal transformer model.


PROJECT OVERVIEW

Visual Question Answering (VQA) is a multimodal AI task that combines computer vision and natural language processing.
The model takes:

an image

a text-based question

and produces a natural language answer.

This project uses ViLT (Vision-and-Language Transformer), which jointly processes visual and textual inputs without relying on separate object detection models.

FEATURES

Upload an image through a web interface

Ask natural language questions about the image

Real-time inference using a pre-trained VQA model

Minimal and beginner-friendly codebase

No cloud APIs or API keys required

TECH STACK

Python

Hugging Face Transformers

ViLT (Vision-and-Language Transformer)

Gradio (Web Interface)

PyTorch

Pillow (PIL)


Install dependencies:

pip install -r requirements.txt

RUNNING THE APPLICATION

Run the following command:

python app.py

After running the script, Gradio will display a local URL.
Open the link in your browser to interact with the application.

EXAMPLE QUESTIONS

What is the person holding?

How many objects are in the image?

What color is the car?

Is it daytime or nighttime?
PROJECT STRUCTURE

visual-question-answering-app/
│
├── app.py
├── requirements.txt
├── README.txt
└── .gitignore

MODEL DETAILS

Model Name: dandelin/vilt-b32-finetuned-vqa
Model Type: Vision-and-Language Transformer (ViLT)
Task: Visual Question Answering
Source: Hugging Face Model Hub